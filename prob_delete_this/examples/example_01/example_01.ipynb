{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer-based language models have been stealing the show (GPT-3, Switch Transformer, Wu Dao)\n",
    "- GPT3 needs 25 days to train on Amazon training platform, Wu Dao needs more than 1000 GPUs to only store its parameters\n",
    "- Parallelism is a framework strategy to tackle the size of large models or improve training efficiency, and distribution is an infrastructure architecture to scale out \n",
    "- Expert parallelism: mix of data parallelism and model parallelism (Chat GPT-3 e.g.)\n",
    "- Distribution scales the parallelism out in the cloud or cluster\n",
    "- Containarization makes it easy to scale nodes, and Kubernetes is a popular container orchestration platform\n",
    "- Each node can have multiple GPUs (or TPUs and other devices -> tensor processing units)\n",
    "- A container manages one or more GPUs. The parallelism can be dispatched across a cluster of distributed GPU containers\n",
    "- Data and weight partitioning stragegies of Google Switch Transformer (2021): each core has a local copy of the model parameters and a portion of the training data\n",
    "- Parallelism = framework strategy\n",
    "- Distribution = infrastructure architecture (criticla but still nascent)\n",
    "- Optimizing memory usage and processing efficiency becomes vital\n",
    "- expensive to traing a large model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parallelism in PyTorch\n",
    "Frameworks: PyTorch Distributed Data Parallel (DDP), SagaMaker Distributed and Horovod\n",
    "1. Create and dispatch copies of the model, one copy per each accelerator (once per training)\n",
    "2. Shards the data and then distributes it to the corresponding devices (in each iteration)\n",
    "3. Aggregates all results together in the backpropagation step  (in each iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Set up distributed system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2497579877.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def setup(rank, world_size):\n",
    "    # todo\n",
    "    \n",
    "    dist.init_process_group(\"tst\", rank=rank, world_size=world_size)\n",
    "    \n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Define the DDP modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_init(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    model = DummyModel().to(rank)\n",
    "    ddp_model = DDP(mode, device_ids=[rank])\n",
    "    #TODO\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "Spawn to run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dummy(dummy_fn, world_size):\n",
    "    mp.spawn(dummy_fn, args=(world_size,), nprocs=world_size, join=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-data-nns-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
